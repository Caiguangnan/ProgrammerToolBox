### 一、Urllib（python内置的HTTP请求库）

#### 1.1 模块

| 模块名             | 描述               |
| ------------------ | ------------------ |
| urllib.request     | 请求模块           |
| urllib.error       | 异常处理模块       |
| urllib.parse       | url解析模块        |
| urllib.robotparser | robots.txt解析模块 |

注意：python2和python3使用urllib的方式不同。

```python
#Python2 导入urllib2，并使用
import urllib2
response = urllib2.urlopen('http://www.baidu.com')

#python3 导入urllib，并使用
import urllib.request
response = urllib.request.urlopen('http://www.baidu.com')
```

#### 1.2 urllib基本用法

**例1：**

```python
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
#方式2：添加表头信息
#req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

**例2：**

```python
import urllib.request
import urllib.parse

#data：绑定参数
data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
#超时时间：1s
timeout = 1

try:
    #访问url链接，并返回响应结果
    response = urllib.request.urlopen(url, data=data, timeout=timeout, cafile=None, capath=None, cadefault=False, context=None)
except urllib.error.URLError as e:
    if isinstance(e.reson,socket.timeout):
        print('TIME OUT')

print(type(response)) #响应类型
print(response.status) #状态码
print(response.getheaders()) #获取响应头
print(response.getheader('Server')) #获取响应头指定参数信息
#直接读取响应的结果
print(response.read())
#访问链接，并将返回的结果设置编码格式
print(response.read().decode('utf-8'))
```

#### 1.3 代理

```python
import urllib.request

proxy_handler = urllib.request.ProxyHandler({
    'http': 'http://127.0.0.1:9743',
    'https': 'https://127.0.0.1:9743'
})
opener = urllib.request.build_opener(proxy_handler)
response = opener.open('http://httpbin.org/get')
print(response.read())
```

#### 1.4 Cookie

```python
import http.cookiejar, urllib.request

cookie = http.cookiejar.CookieJar() #相当于对象
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:   #输出cookie键值对
    print(item.name+"="+item.value) 
```

#### **1.5 保存cookie**

**方式一**

```python
import http.cookiejar, urllib.request
filename = "cookie.txt"
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

**方式二**

```python
import http.cookiejar, urllib.request
filename = 'cookie.txt'
cookie = http.cookiejar.LWPCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

#### 1.6 读取cookie文件

```python
import http.cookiejar, urllib.request
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
print(response.read().decode('utf-8'))
```



### 二、异常处理

#### 2.1 单个异常

```python
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```

#### 2.2 多个异常【子类在前，父类在后】

```python
from urllib import request, error

try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
```

### 三、URL解析

#### 3.1 分割网址

```python
from urllib.parse import urlparse

#scheme：网址的协议类型
#allow_fragments：网址中?后面的参数，是否合并到前一个划分参数的里面
url = 'http://www.baidu.com/index.html;user?id=5#comment'
result = urlparse(url,scheme='http', allow_fragments=True)
print(type(result), result)
```

#### 3.2 合并网址

方式一（urlunparse）

```python
from urllib.parse import urlunparse

data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
```

方式二（urljoin）

```python
from urllib.parse import urljoin

print(urljoin('http://www.baidu.com', 'FAQ.html'))
print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
print(urljoin('http://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))
```

方式三（urlencode）

```python
from urllib.parse import urlencode

params = {
    'name': 'germey',
    'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```















